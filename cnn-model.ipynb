{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2906830,"sourceType":"datasetVersion","datasetId":1781639}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport xml.etree.ElementTree as ET\nimport cv2\nimport io\nimport zipfile\nimport requests\nfrom pathlib import Path\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.optim as optim\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T07:38:29.258910Z","iopub.execute_input":"2025-07-01T07:38:29.259248Z","iopub.status.idle":"2025-07-01T07:38:39.374358Z","shell.execute_reply.started":"2025-07-01T07:38:29.259222Z","shell.execute_reply":"2025-07-01T07:38:39.373563Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Step 1: Download and extract ZIP\nurl = \"https://github.com/ravipeddi-05/EEN1072-ASSIGNMENT/archive/refs/heads/main.zip\"\nextract_to = \"cnn_dataset\"\n\nresponse = requests.get(url)\nwith zipfile.ZipFile(io.BytesIO(response.content)) as z:\n    z.extractall(extract_to)\n\n# Step 2: Define paths\nbase_path = Path(extract_to) / \"EEN1072-ASSIGNMENT-main\" / \"CNN dataset\"\nimages_path = base_path / \"images\"\nannotations_path = base_path / \"annotations\"\n\n# Step 3: Load images and corresponding annotations\ndataset = []\n\nfor image_file in images_path.glob(\"*.png\"):\n    image_id = image_file.stem  # e.g., 'img1'\n    annotation_file = annotations_path / f\"{image_id}.xml\"\n\n    if annotation_file.exists():\n        # Load image\n        image = Image.open(image_file).copy()\n\n        # Parse XML annotation\n        tree = ET.parse(annotation_file)\n        root = tree.getroot()\n\n        objects = []\n        for obj in root.findall(\"object\"):\n            label = obj.find(\"name\").text\n            bbox = obj.find(\"bndbox\")\n            box = {\n                \"label\": label,\n                \"xmin\": int(bbox.find(\"xmin\").text),\n                \"ymin\": int(bbox.find(\"ymin\").text),\n                \"xmax\": int(bbox.find(\"xmax\").text),\n                \"ymax\": int(bbox.find(\"ymax\").text)\n            }\n            objects.append(box)\n\n        # Append paired image and annotation\n        dataset.append({\n            \"image_id\": image_id,\n            \"image\": image,\n            \"annotations\": objects\n        })\n\nprint(f\"✅ Loaded {len(dataset)} image-annotation pairs from CNN dataset.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T08:09:10.604269Z","iopub.execute_input":"2025-07-01T08:09:10.604866Z","iopub.status.idle":"2025-07-01T08:10:14.168449Z","shell.execute_reply.started":"2025-07-01T08:09:10.604840Z","shell.execute_reply":"2025-07-01T08:10:14.167743Z"}},"outputs":[{"name":"stdout","text":"✅ Loaded 2330 image-annotation pairs from CNN dataset.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# # Dataset uploading\n# DATASET_DIR = '/kaggle/input/leaf-types-object-detection'\n# IMAGES_DIR = os.path.join(DATASET_DIR, 'images')\n# ANNOTATIONS_DIR = os.path.join(DATASET_DIR, 'annotations')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:28:02.924525Z","iopub.execute_input":"2025-06-30T11:28:02.925260Z","iopub.status.idle":"2025-06-30T11:28:02.928978Z","shell.execute_reply.started":"2025-06-30T11:28:02.925237Z","shell.execute_reply":"2025-06-30T11:28:02.928326Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#Extracting bounding boxes\ndef parse_voc_annotation(xml_file):\n    tree = ET.parse(xml_file)\n    root = tree.getroot()\n    boxes = []\n    for obj in root.findall(\"object\"):\n        label = obj.find(\"name\").text\n        bbox = obj.find(\"bndbox\")\n        box = [\n            int(bbox.find(\"xmin\").text),\n            int(bbox.find(\"ymin\").text),\n            int(bbox.find(\"xmax\").text),\n            int(bbox.find(\"ymax\").text),\n            label\n        ]\n        boxes.append(box)\n    return boxes\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:28:05.282988Z","iopub.execute_input":"2025-06-30T11:28:05.283499Z","iopub.status.idle":"2025-06-30T11:28:05.287727Z","shell.execute_reply.started":"2025-06-30T11:28:05.283475Z","shell.execute_reply":"2025-06-30T11:28:05.287107Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#Splitting Data\nimage_files = sorted([\n    f for f in os.listdir(IMAGES_DIR)\n    if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n])\n\nimage_files = sorted([f for f in os.listdir(IMAGES_DIR) if f.endswith('.png')])\ntrain_imgs, test_imgs = train_test_split(image_files, test_size=0.1, random_state=42)\ntrain_imgs, val_imgs = train_test_split(train_imgs, test_size=0.1, random_state=42)\n\nprint(f\"Train: {len(train_imgs)} | Val: {len(val_imgs)} | Test: {len(test_imgs)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:28:07.852018Z","iopub.execute_input":"2025-06-30T11:28:07.852580Z","iopub.status.idle":"2025-06-30T11:28:07.886272Z","shell.execute_reply.started":"2025-06-30T11:28:07.852553Z","shell.execute_reply":"2025-06-30T11:28:07.885728Z"}},"outputs":[{"name":"stdout","text":"Train: 1887 | Val: 210 | Test: 233\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\nclass LeafDataset(Dataset):\n    def __init__(self, image_filenames, transform=None):\n        self.image_filenames = image_filenames\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n    def __getitem__(self, idx):\n        img_name = self.image_filenames[idx]\n        img_path = os.path.join(IMAGES_DIR, img_name)\n        xml_filename = get_xml_filename(img_name)\n        xml_path = os.path.join(ANNOTATIONS_DIR, xml_filename)\n\n\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        boxes = parse_voc_annotation(xml_path)\n\n        # Skip images with no boxes or inconsistent shapes\n        if len(boxes) == 0:\n            box = [0, 0, 0, 0]\n        else:\n            box = boxes[0][:4]  # Take only first box\n\n        label_tensor = torch.tensor(box, dtype=torch.float32)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label_tensor\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:28:09.812151Z","iopub.execute_input":"2025-06-30T11:28:09.812426Z","iopub.status.idle":"2025-06-30T11:28:09.818111Z","shell.execute_reply.started":"2025-06-30T11:28:09.812404Z","shell.execute_reply":"2025-06-30T11:28:09.817501Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n\ntrain_set = LeafDataset(train_imgs, transform=transform)\nval_set = LeafDataset(val_imgs, transform=transform)\ntest_set = LeafDataset(test_imgs, transform=transform)\n\ntrain_loader = DataLoader(train_set, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=8)\ntest_loader = DataLoader(test_set, batch_size=8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:28:15.674128Z","iopub.execute_input":"2025-06-30T11:28:15.674403Z","iopub.status.idle":"2025-06-30T11:28:15.679538Z","shell.execute_reply.started":"2025-06-30T11:28:15.674381Z","shell.execute_reply":"2025-06-30T11:28:15.678747Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class LeafCNN(nn.Module):\n    def __init__(self):\n        super(LeafCNN, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n        )\n        self.regressor = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128 * 28 * 28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 4)  # Predict one bounding box (x1, y1, x2, y2)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.regressor(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:28:17.682701Z","iopub.execute_input":"2025-06-30T11:28:17.683022Z","iopub.status.idle":"2025-06-30T11:28:17.688544Z","shell.execute_reply.started":"2025-06-30T11:28:17.682999Z","shell.execute_reply":"2025-06-30T11:28:17.687941Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def calculate_iou(box1, box2):\n    \"\"\"\n    box = [xmin, ymin, xmax, ymax]\n    \"\"\"\n    xA = max(box1[0], box2[0])\n    yA = max(box1[1], box2[1])\n    xB = min(box1[2], box2[2])\n    yB = min(box1[3], box2[3])\n\n    inter_area = max(0, xB - xA) * max(0, yB - yA)\n    box1_area = max(0, (box1[2] - box1[0])) * max(0, (box1[3] - box1[1]))\n    box2_area = max(0, (box2[2] - box2[0])) * max(0, (box2[3] - box2[1]))\n\n    union_area = box1_area + box2_area - inter_area\n    iou = inter_area / union_area if union_area != 0 else 0\n    return iou\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:28:21.721724Z","iopub.execute_input":"2025-06-30T11:28:21.722028Z","iopub.status.idle":"2025-06-30T11:28:21.727186Z","shell.execute_reply.started":"2025-06-30T11:28:21.722005Z","shell.execute_reply":"2025-06-30T11:28:21.726344Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def evaluate_iou(model, loader):\n    model.eval()\n    total_iou = 0\n    count = 0\n\n    with torch.no_grad():\n        for images, targets in loader:\n            images = images.to(device)\n            targets = targets.to(device)\n\n            outputs = model(images)  # Should be [batch_size, 4]\n            if outputs.ndim == 1:\n                outputs = outputs.unsqueeze(0)\n            if targets.ndim == 1:\n                targets = targets.unsqueeze(0)\n\n            for pred, actual in zip(outputs, targets):\n                pred_box = pred.cpu().numpy()\n                true_box = actual.cpu().numpy()\n                if len(pred_box) == 4 and len(true_box) == 4:\n                    iou = calculate_iou(pred_box, true_box)\n                    total_iou += iou\n                    count += 1\n\n    avg_iou = total_iou / count if count > 0 else 0\n    print(f\"📊 Average IoU on test set: {avg_iou:.4f}\")\n    return avg_iou\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:28:24.804084Z","iopub.execute_input":"2025-06-30T11:28:24.804667Z","iopub.status.idle":"2025-06-30T11:28:24.810103Z","shell.execute_reply.started":"2025-06-30T11:28:24.804641Z","shell.execute_reply":"2025-06-30T11:28:24.809521Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def get_xml_filename(img_name):\n    \"\"\"\n    Given an image filename (e.g., leaf1.jpg), returns the corresponding XML annotation filename (e.g., leaf1.xml).\n    \"\"\"\n    base = os.path.splitext(img_name)[0]  # Remove file extension\n    return base + '.xml'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:29:19.740182Z","iopub.execute_input":"2025-06-30T11:29:19.740491Z","iopub.status.idle":"2025-06-30T11:29:19.744377Z","shell.execute_reply.started":"2025-06-30T11:29:19.740469Z","shell.execute_reply":"2025-06-30T11:29:19.743643Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"model = LeafCNN()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\ndef train(model, loader, val_loader, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for images, targets in train_loader:\n            images = images.to(device)\n            targets = targets.to(device)  # now shape [batch_size, 4]\n            \n            outputs = model(images)\n            loss = criterion(outputs, targets)\n        \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(loader):.4f}\")\n\ntrain(model, train_loader, val_loader)\n\ndef evaluate(model, loader):\n    model.eval()\n    with torch.no_grad():\n        for images, targets in loader:\n            images, targets = images.to(device), targets[:, 0].to(device)\n            outputs = model(images)\n            print(\"Prediction:\", outputs[0].cpu().numpy())\n            print(\"Actual:\", targets[0].cpu().numpy())\n            break\n\navg_iou = evaluate_iou(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:29:22.284255Z","iopub.execute_input":"2025-06-30T11:29:22.284788Z","iopub.status.idle":"2025-06-30T11:31:28.429679Z","shell.execute_reply.started":"2025-06-30T11:29:22.284750Z","shell.execute_reply":"2025-06-30T11:31:28.428930Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/10], Loss: 573.5683\nEpoch [2/10], Loss: 462.5101\nEpoch [3/10], Loss: 435.5832\nEpoch [4/10], Loss: 410.8650\nEpoch [5/10], Loss: 403.0633\nEpoch [6/10], Loss: 369.0020\nEpoch [7/10], Loss: 341.4133\nEpoch [8/10], Loss: 296.0272\nEpoch [9/10], Loss: 240.5239\nEpoch [10/10], Loss: 190.1183\n📊 Average IoU on test set: 0.3245\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"class LeafCNN_V1(nn.Module):\n    def __init__(self):\n        super(LeafCNN_V1, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2)\n        )\n        self.regressor = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128 * 28 * 28, 512),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(512, 4)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        return self.regressor(x)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:32:50.616171Z","iopub.execute_input":"2025-06-30T11:32:50.616441Z","iopub.status.idle":"2025-06-30T11:32:50.624418Z","shell.execute_reply.started":"2025-06-30T11:32:50.616422Z","shell.execute_reply":"2025-06-30T11:32:50.623696Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"class LeafCNN_V2(nn.Module):\n    def __init__(self):\n        super(LeafCNN_V2, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2)\n        )\n        self.regressor = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(256 * 14 * 14, 512),\n            nn.ReLU(),\n            nn.Linear(512, 4)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        return self.regressor(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:59:01.972566Z","iopub.execute_input":"2025-06-30T06:59:01.973422Z","iopub.status.idle":"2025-06-30T06:59:01.978823Z","shell.execute_reply.started":"2025-06-30T06:59:01.973391Z","shell.execute_reply":"2025-06-30T06:59:01.978082Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"from itertools import product\n\n# Define search space\nlr_list = [0.001, 0.0005]\nbatch_sizes = [8, 16]\nhidden_sizes = [256, 512]\n\nsearch_space = list(product(lr_list, batch_sizes, hidden_sizes))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:59:06.940088Z","iopub.execute_input":"2025-06-30T06:59:06.940384Z","iopub.status.idle":"2025-06-30T06:59:06.944367Z","shell.execute_reply.started":"2025-06-30T06:59:06.940366Z","shell.execute_reply":"2025-06-30T06:59:06.943571Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"class TunedLeafCNN(nn.Module):\n    def __init__(self, hidden_size):\n        super(TunedLeafCNN, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n        )\n        self.regressor = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128 * 28 * 28, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 4)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        return self.regressor(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T06:59:10.187096Z","iopub.execute_input":"2025-06-30T06:59:10.187700Z","iopub.status.idle":"2025-06-30T06:59:10.192831Z","shell.execute_reply.started":"2025-06-30T06:59:10.187675Z","shell.execute_reply":"2025-06-30T06:59:10.192116Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def run_tuning(search_space, train_imgs, val_imgs):\n    best_loss = float('inf')\n    best_params = None\n    best_model = None\n\n    for lr, batch_size, hidden_size in search_space:\n        print(f\"\\n🔧 Testing config: LR={lr}, Batch={batch_size}, Hidden={hidden_size}\")\n\n        # Load data\n        train_set = LeafDataset(train_imgs, transform=transform)\n        val_set = LeafDataset(val_imgs, transform=transform)\n        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(val_set, batch_size=batch_size)\n\n        # Model + Optimizer\n        model = TunedLeafCNN(hidden_size).to(device)\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        criterion = nn.SmoothL1Loss()  # Can replace with IoU loss later\n\n        best_val_loss_for_config = float('inf')\n\n        for epoch in range(10):  # ⏱️ Increased to 30 epochs\n            model.train()\n            train_loss = 0\n\n            for images, targets in train_loader:\n                images, targets = images.to(device), targets.to(device)\n                outputs = model(images)\n\n                loss = criterion(outputs, targets)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                train_loss += loss.item()\n\n            avg_train_loss = train_loss / len(train_loader)\n\n            # Validation per epoch\n            model.eval()\n            val_loss = 0\n            with torch.no_grad():\n                for images, targets in val_loader:\n                    images, targets = images.to(device), targets.to(device)\n                    outputs = model(images)\n                    loss = criterion(outputs, targets)\n                    val_loss += loss.item()\n            avg_val_loss = val_loss / len(val_loader)\n\n            # Optional: log per epoch\n            print(f\"Epoch {epoch+1:02d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n\n            # Save best model for this config\n            if avg_val_loss < best_val_loss_for_config:\n                best_val_loss_for_config = avg_val_loss\n                current_best_model = model\n\n        # Track best model across all configs\n        print(f\"🔎 Final Val Loss for config: {best_val_loss_for_config:.4f}\")\n        if best_val_loss_for_config < best_loss:\n            best_loss = best_val_loss_for_config\n            best_params = (lr, batch_size, hidden_size)\n            best_model = current_best_model\n\n    print(f\"\\n✅ Best Config Overall → LR={best_params[0]}, Batch={best_params[1]}, Hidden={best_params[2]}\")\n    return best_model, best_params\n\nbest_model, best_params = run_tuning(search_space, train_imgs, val_imgs)\navg_iou = evaluate_iou(best_model, test_loader)\nprint(f\"✅ Final Average IoU: {avg_iou:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T08:33:40.451889Z","iopub.execute_input":"2025-06-30T08:33:40.452596Z","iopub.status.idle":"2025-06-30T08:49:10.707107Z","shell.execute_reply.started":"2025-06-30T08:33:40.452565Z","shell.execute_reply":"2025-06-30T08:49:10.706489Z"}},"outputs":[{"name":"stdout","text":"\n🔧 Testing config: LR=0.001, Batch=8, Hidden=256\nEpoch 01 | Train Loss: 18.6849 | Val Loss: 17.5073\nEpoch 02 | Train Loss: 17.0696 | Val Loss: 16.1190\nEpoch 03 | Train Loss: 16.3890 | Val Loss: 15.4930\nEpoch 04 | Train Loss: 15.8385 | Val Loss: 15.2044\nEpoch 05 | Train Loss: 15.5713 | Val Loss: 16.2533\nEpoch 06 | Train Loss: 15.2611 | Val Loss: 15.1906\nEpoch 07 | Train Loss: 14.7214 | Val Loss: 14.6279\nEpoch 08 | Train Loss: 14.5746 | Val Loss: 14.3078\nEpoch 09 | Train Loss: 14.2589 | Val Loss: 14.6688\nEpoch 10 | Train Loss: 13.4664 | Val Loss: 16.0170\n🔎 Final Val Loss for config: 14.3078\n\n🔧 Testing config: LR=0.001, Batch=8, Hidden=512\nEpoch 01 | Train Loss: 18.4107 | Val Loss: 17.4747\nEpoch 02 | Train Loss: 17.2358 | Val Loss: 15.7940\nEpoch 03 | Train Loss: 16.4915 | Val Loss: 15.2623\nEpoch 04 | Train Loss: 15.9545 | Val Loss: 15.1991\nEpoch 05 | Train Loss: 15.9219 | Val Loss: 15.3399\nEpoch 06 | Train Loss: 15.3183 | Val Loss: 16.4173\nEpoch 07 | Train Loss: 15.0801 | Val Loss: 14.6139\nEpoch 08 | Train Loss: 14.5226 | Val Loss: 14.9250\nEpoch 09 | Train Loss: 14.1211 | Val Loss: 14.4981\nEpoch 10 | Train Loss: 13.6341 | Val Loss: 14.9058\n🔎 Final Val Loss for config: 14.4981\n\n🔧 Testing config: LR=0.001, Batch=16, Hidden=256\nEpoch 01 | Train Loss: 18.8291 | Val Loss: 17.0225\nEpoch 02 | Train Loss: 17.1233 | Val Loss: 16.3883\nEpoch 03 | Train Loss: 17.0383 | Val Loss: 15.6787\nEpoch 04 | Train Loss: 16.0119 | Val Loss: 16.7173\nEpoch 05 | Train Loss: 15.5927 | Val Loss: 14.5125\nEpoch 06 | Train Loss: 14.9810 | Val Loss: 13.9724\nEpoch 07 | Train Loss: 14.5432 | Val Loss: 14.3650\nEpoch 08 | Train Loss: 14.1358 | Val Loss: 13.8474\nEpoch 09 | Train Loss: 13.4389 | Val Loss: 13.9869\nEpoch 10 | Train Loss: 12.8254 | Val Loss: 14.2561\n🔎 Final Val Loss for config: 13.8474\n\n🔧 Testing config: LR=0.001, Batch=16, Hidden=512\nEpoch 01 | Train Loss: 18.6380 | Val Loss: 16.8572\nEpoch 02 | Train Loss: 17.2292 | Val Loss: 15.9643\nEpoch 03 | Train Loss: 16.3781 | Val Loss: 15.5862\nEpoch 04 | Train Loss: 15.9232 | Val Loss: 15.3291\nEpoch 05 | Train Loss: 15.6168 | Val Loss: 14.7361\nEpoch 06 | Train Loss: 15.0922 | Val Loss: 15.2853\nEpoch 07 | Train Loss: 14.9907 | Val Loss: 14.8504\nEpoch 08 | Train Loss: 14.5293 | Val Loss: 14.3589\nEpoch 09 | Train Loss: 13.7790 | Val Loss: 14.2137\nEpoch 10 | Train Loss: 13.2625 | Val Loss: 15.2475\n🔎 Final Val Loss for config: 14.2137\n\n🔧 Testing config: LR=0.0005, Batch=8, Hidden=256\nEpoch 01 | Train Loss: 18.3209 | Val Loss: 18.3131\nEpoch 02 | Train Loss: 17.2575 | Val Loss: 16.5437\nEpoch 03 | Train Loss: 16.5249 | Val Loss: 15.3413\nEpoch 04 | Train Loss: 15.6094 | Val Loss: 16.3574\nEpoch 05 | Train Loss: 15.4574 | Val Loss: 15.0581\nEpoch 06 | Train Loss: 14.8054 | Val Loss: 15.1538\nEpoch 07 | Train Loss: 14.7902 | Val Loss: 14.3786\nEpoch 08 | Train Loss: 14.2636 | Val Loss: 14.5624\nEpoch 09 | Train Loss: 14.0091 | Val Loss: 14.2356\nEpoch 10 | Train Loss: 13.4545 | Val Loss: 14.1718\n🔎 Final Val Loss for config: 14.1718\n\n🔧 Testing config: LR=0.0005, Batch=8, Hidden=512\nEpoch 01 | Train Loss: 18.3809 | Val Loss: 18.5841\nEpoch 02 | Train Loss: 17.2390 | Val Loss: 16.7431\nEpoch 03 | Train Loss: 16.6661 | Val Loss: 16.5392\nEpoch 04 | Train Loss: 16.1473 | Val Loss: 15.0441\nEpoch 05 | Train Loss: 15.5073 | Val Loss: 15.6362\nEpoch 06 | Train Loss: 15.1005 | Val Loss: 15.9296\nEpoch 07 | Train Loss: 14.6004 | Val Loss: 14.7970\nEpoch 08 | Train Loss: 14.0403 | Val Loss: 14.4294\nEpoch 09 | Train Loss: 13.4537 | Val Loss: 14.0639\nEpoch 10 | Train Loss: 12.9723 | Val Loss: 14.1969\n🔎 Final Val Loss for config: 14.0639\n\n🔧 Testing config: LR=0.0005, Batch=16, Hidden=256\nEpoch 01 | Train Loss: 19.6754 | Val Loss: 17.8467\nEpoch 02 | Train Loss: 17.3051 | Val Loss: 16.4691\nEpoch 03 | Train Loss: 16.9048 | Val Loss: 17.1013\nEpoch 04 | Train Loss: 16.4187 | Val Loss: 14.9241\nEpoch 05 | Train Loss: 15.6788 | Val Loss: 14.3826\nEpoch 06 | Train Loss: 15.4496 | Val Loss: 14.1383\nEpoch 07 | Train Loss: 14.6821 | Val Loss: 14.0849\nEpoch 08 | Train Loss: 14.5262 | Val Loss: 14.2332\nEpoch 09 | Train Loss: 13.8843 | Val Loss: 13.7089\nEpoch 10 | Train Loss: 13.4867 | Val Loss: 14.3020\n🔎 Final Val Loss for config: 13.7089\n\n🔧 Testing config: LR=0.0005, Batch=16, Hidden=512\nEpoch 01 | Train Loss: 19.1936 | Val Loss: 17.0705\nEpoch 02 | Train Loss: 17.3504 | Val Loss: 16.9587\nEpoch 03 | Train Loss: 17.1184 | Val Loss: 15.9418\nEpoch 04 | Train Loss: 16.2266 | Val Loss: 15.1252\nEpoch 05 | Train Loss: 15.8914 | Val Loss: 15.0941\nEpoch 06 | Train Loss: 15.4941 | Val Loss: 14.9500\nEpoch 07 | Train Loss: 15.3967 | Val Loss: 14.9036\nEpoch 08 | Train Loss: 14.7846 | Val Loss: 14.4615\nEpoch 09 | Train Loss: 14.2404 | Val Loss: 14.7401\nEpoch 10 | Train Loss: 14.0554 | Val Loss: 15.2530\n🔎 Final Val Loss for config: 14.4615\n\n✅ Best Config Overall → LR=0.0005, Batch=16, Hidden=256\n📊 Average IoU on test set: 0.3205\n✅ Final Average IoU: 0.3205\n","output_type":"stream"}],"execution_count":50}]}